# SparseVLM è®­ç»ƒç»†èŠ‚ï¼šå‚æ•°æ•°é‡ä¸è¾“å…¥è¾“å‡º

## ğŸ“‹ ç›®å½•
1. [è®­ç»ƒå‚æ•°æ•°é‡](#è®­ç»ƒå‚æ•°æ•°é‡)
2. [æ¨¡å‹è¾“å…¥è¾“å‡ºæ ¼å¼](#æ¨¡å‹è¾“å…¥è¾“å‡ºæ ¼å¼)
3. [è®­ç»ƒæµç¨‹è¯¦è§£](#è®­ç»ƒæµç¨‹è¯¦è§£)
4. [ä»£ç ç¤ºä¾‹](#ä»£ç ç¤ºä¾‹)

---

## 1. è®­ç»ƒå‚æ•°æ•°é‡

### 1.1 å‚æ•°å†»ç»“ç­–ç•¥

SparseVLMæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼ï¼Œæ ¹æ®é…ç½®ä¸åŒï¼Œå¯è®­ç»ƒå‚æ•°æ•°é‡å·®å¼‚å¾ˆå¤§ï¼š

#### **æ¨¡å¼1: å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰**
- **å¯è®­ç»ƒå‚æ•°**: æ•´ä¸ªæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼ˆçº¦7Bæˆ–13Bï¼‰
- **é…ç½®**: `freeze_backbone=False`ï¼ˆé»˜è®¤ï¼‰
- **å‚æ•°é‡**:
  - LLaVA-1.5-7B: **~7Bå‚æ•°**
  - LLaVA-1.5-13B: **~13Bå‚æ•°**
- **é€‚ç”¨åœºæ™¯**: å®Œæ•´è®­ç»ƒï¼Œæ•ˆæœæœ€å¥½ä½†èµ„æºæ¶ˆè€—æœ€å¤§

#### **æ¨¡å¼2: ä»…è®­ç»ƒæŠ•å½±å±‚ï¼ˆProjector-onlyï¼‰**
- **å¯è®­ç»ƒå‚æ•°**: ä»…`mm_projector`ï¼ˆè§†è§‰-è¯­è¨€æŠ•å½±å±‚ï¼‰
- **é…ç½®**: `tune_mm_mlp_adapter=True`
- **å‚æ•°é‡**: çº¦**2-4Må‚æ•°**ï¼ˆå–å†³äºæŠ•å½±å±‚é…ç½®ï¼‰
- **é€‚ç”¨åœºæ™¯**: é¢„è®­ç»ƒé˜¶æ®µï¼Œèµ„æºæ¶ˆè€—æœ€å°

#### **æ¨¡å¼3: å†»ç»“ä¸»å¹²ç½‘ç»œï¼ˆFreeze Backboneï¼‰**
- **å¯è®­ç»ƒå‚æ•°**: é™¤LLMä¸»å¹²å¤–çš„æ‰€æœ‰å‚æ•°
- **é…ç½®**: `freeze_backbone=True`
- **å‚æ•°é‡**: çº¦**10-50Må‚æ•°**ï¼ˆä¸»è¦æ˜¯æŠ•å½±å±‚å’Œæ³¨æ„åŠ›æœºåˆ¶ç›¸å…³ï¼‰
- **é€‚ç”¨åœºæ™¯**: å¿«é€Ÿå¾®è°ƒï¼Œä¿æŒLLMæƒé‡ä¸å˜

#### **æ¨¡å¼4: LoRAå¾®è°ƒ**
- **å¯è®­ç»ƒå‚æ•°**: LoRAé€‚é…å™¨å‚æ•°
- **é…ç½®**: `lora_enable=True`, `lora_r=64`, `lora_alpha=16`
- **å‚æ•°é‡**: çº¦**10-100Må‚æ•°**ï¼ˆå–å†³äºLoRA rankï¼‰
- **é€‚ç”¨åœºæ™¯**: å‚æ•°é«˜æ•ˆå¾®è°ƒ

### 1.2 Teacheræ¨¡å‹

- **Teacheræ¨¡å‹**: å®Œå…¨å†»ç»“ï¼ˆ`requires_grad=False`ï¼‰
- **ç”¨é€”**: æä¾›çŸ¥è¯†è’¸é¦çš„ç›‘ç£ä¿¡å·
- **å‚æ•°é‡**: ä¸Studentæ¨¡å‹ç›¸åŒï¼ˆ7Bæˆ–13Bï¼‰ï¼Œä½†ä¸å‚ä¸æ¢¯åº¦æ›´æ–°

### 1.3 å®é™…è®­ç»ƒå‚æ•°ç»Ÿè®¡

```python
# ä»£ç ä½ç½®: llava/train/sparse_train.py

# Teacheræ¨¡å‹å†»ç»“
teacher_model.model.requires_grad_(False)  # ç¬¬863è¡Œ

# å¯é€‰ï¼šå†»ç»“ä¸»å¹²ç½‘ç»œ
if model_args.freeze_backbone:
    model.model.requires_grad_(False)  # ç¬¬866è¡Œ

# å¯é€‰ï¼šä»…è®­ç»ƒæŠ•å½±å±‚
if model_args.tune_mm_mlp_adapter:
    model.requires_grad_(False)
    for p in model.get_model().mm_projector.parameters():
        p.requires_grad = True  # ç¬¬954-955è¡Œ
```

### 1.4 å‚æ•°æ•°é‡ä¼°ç®—

| è®­ç»ƒæ¨¡å¼ | 7Bæ¨¡å‹å¯è®­ç»ƒå‚æ•° | 13Bæ¨¡å‹å¯è®­ç»ƒå‚æ•° | æ˜¾å­˜éœ€æ±‚ï¼ˆå•å¡ï¼‰ |
|---------|-----------------|------------------|-----------------|
| å…¨å‚æ•°å¾®è°ƒ | ~7B | ~13B | 40GB+ |
| å†»ç»“ä¸»å¹² | ~50M | ~100M | 20GB+ |
| ä»…æŠ•å½±å±‚ | ~4M | ~8M | 15GB+ |
| LoRA (r=64) | ~50M | ~100M | 18GB+ |

**æ³¨æ„**: å®é™…æ˜¾å­˜éœ€æ±‚è¿˜å–å†³äºbatch sizeã€åºåˆ—é•¿åº¦ç­‰å› ç´ ã€‚

---

## 2. æ¨¡å‹è¾“å…¥è¾“å‡ºæ ¼å¼

### 2.1 è®­ç»ƒè¾“å…¥ï¼ˆTraining Inputï¼‰

#### **æ•°æ®æ ¼å¼**
```python
# è¾“å…¥å­—å…¸ç»“æ„
inputs = {
    'input_ids': torch.LongTensor,      # [B, L] Token IDs
    'images': torch.FloatTensor,         # [B, 3, H, W] æˆ– List[Tensor]
    'attention_mask': torch.BoolTensor,  # [B, L] æ³¨æ„åŠ›æ©ç 
    'labels': torch.LongTensor,          # [B, L] æ ‡ç­¾ï¼ˆç”¨äºè®¡ç®—lossï¼‰
    'image_sizes': List[List[int]],      # å›¾åƒå°ºå¯¸åˆ—è¡¨
}
```

#### **å…·ä½“ç»´åº¦ç¤ºä¾‹**
```python
# ç¤ºä¾‹ï¼šå•ä¸ªæ ·æœ¬
input_ids: torch.Size([1, 668])           # 668ä¸ªtoken
images: torch.Size([1, 3, 336, 336])       # å•å¼ å›¾åƒ
attention_mask: torch.Size([1, 668])      # æ³¨æ„åŠ›æ©ç 
labels: torch.Size([1, 668])              # æ ‡ç­¾ï¼ˆIGNORE_INDEXç”¨äºmaskï¼‰

# ç»è¿‡prepare_sparse_inputs_labels_for_multimodalå¤„ç†å
inputs_embeds: torch.Size([1, 668, 4096]) # åµŒå…¥å‘é‡ï¼ˆåŒ…å«è§†è§‰ç‰¹å¾ï¼‰
```

#### **æ•°æ®é¢„å¤„ç†æµç¨‹**
1. **å›¾åƒç¼–ç **: å›¾åƒ â†’ CLIPè§†è§‰ç¼–ç å™¨ â†’ è§†è§‰ç‰¹å¾ `[B, 576, 4096]`ï¼ˆ576ä¸ªè§†è§‰tokenï¼‰
2. **æ–‡æœ¬Tokenization**: æ–‡æœ¬ â†’ Tokenizer â†’ `input_ids`
3. **ç‰¹å¾èåˆ**: å°†è§†è§‰ç‰¹å¾æ’å…¥åˆ°æ–‡æœ¬tokenåºåˆ—ä¸­
4. **æœ€ç»ˆè¾“å…¥**: `inputs_embeds` = æ–‡æœ¬åµŒå…¥ + è§†è§‰åµŒå…¥

### 2.2 æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰

#### **è¾“å…¥å¤„ç†**
```python
# ä»£ç ä½ç½®: llava/model/llava_arch.py:325-509

def prepare_sparse_inputs_labels_for_multimodal(
    self, input_ids, position_ids, attention_mask, past_key_values, labels,
    images, image_sizes=None
):
    # 1. ç¼–ç å›¾åƒ
    image_features = self.encode_images(images)  # [B, 576, 4096]
    
    # 2. èåˆæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾
    new_input_embeds = []  # åŒ…å«æ–‡æœ¬åµŒå…¥å’Œè§†è§‰åµŒå…¥
    
    # 3. è¿”å›å¤„ç†åçš„è¾“å…¥
    return (
        None,                    # input_ids (è®¾ä¸ºNoneï¼Œä½¿ç”¨inputs_embeds)
        position_ids,           # [B, L]
        attention_mask,         # [B, L]
        past_key_values,        # KV cache
        new_input_embeds,        # [B, L, 4096] èåˆåçš„åµŒå…¥
        new_labels,              # [B, L]
        image_shape,             # 576 (è§†è§‰tokenæ•°é‡)
        token_length_list,       # æ¯ä¸ªæ ·æœ¬çš„å®é™…é•¿åº¦
        pre_prompt_length_list,  # prompté•¿åº¦åˆ—è¡¨
    )
```

#### **æ¨¡å‹è¾“å‡º**
```python
# ä»£ç ä½ç½®: llava/model/language_model/modelling_sparse_llama.py:384-391

# è®­ç»ƒæ—¶è¾“å‡º
outputs = (
    prev_decision,              # torch.Tensor [B, L] Tokené€‰æ‹©å†³ç­–ï¼ˆ0/1ï¼‰
    out_pred_prob,                # é¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
    BaseModelOutputWithPast(
        last_hidden_state=hidden_states,  # [B, L_new, 4096] ç¨€ç–åŒ–åçš„éšè—çŠ¶æ€
        past_key_values=next_cache,       # KV cache
        hidden_states=all_hidden_states,  # æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€
        attentions=all_self_attns,        # æ³¨æ„åŠ›æƒé‡
    )
)
```

### 2.3 æŸå¤±è®¡ç®—ï¼ˆLoss Computationï¼‰

#### **æŸå¤±å‡½æ•°ç»„æˆ**
```python
# ä»£ç ä½ç½®: llava/train/sparse_llava_trainer.py:430-492

def compute_loss(self, model, inputs, return_outputs=False):
    # 1. Studentæ¨¡å‹å‰å‘ä¼ æ’­
    sparse_outputs = model(**inputs)
    prev_decision, out_pred_prob, outputs = sparse_outputs
    hidden_states = outputs.hidden_states  # Studentçš„éšè—çŠ¶æ€
    
    # 2. Teacheræ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
    with torch.no_grad():
        teacher_outputs = self.teacher_model(**inputs)
        teacher_hidden_states = teacher_outputs.hidden_states[-1]
    
    # 3. ç‰¹å¾å¯¹é½æŸå¤±ï¼ˆFeature Alignment Lossï¼‰
    B, L, C = hidden_states.shape
    bool_mask = prev_decision.reshape(B*L) > 0.5  # ä¿ç•™çš„token
    hidden_states = hidden_states.reshape(B*L, C)
    teacher_hidden_states = teacher_hidden_states.reshape(B*L, C)
    
    # åªå¯¹é½ä¿ç•™çš„token
    hidden_states = hidden_states[bool_mask]
    teacher_hidden_states = teacher_hidden_states[bool_mask]
    align_loss = torch.pow(hidden_states - teacher_hidden_states, 2).mean()
    
    # 4. è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆLanguage Modeling Lossï¼‰
    if labels is not None:
        loss = self.label_smoother(outputs, labels, shift_labels=True)
    
    # 5. æ€»æŸå¤±ï¼ˆå¯é€‰ï¼šæ·»åŠ align_lossï¼‰
    # total_loss = loss + alpha * align_loss
    
    return (total_loss, outputs) if return_outputs else total_loss
```

#### **æŸå¤±å‡½æ•°ç±»å‹**
1. **è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆLM Lossï¼‰**: æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªtoken
2. **ç‰¹å¾å¯¹é½æŸå¤±ï¼ˆAlignment Lossï¼‰**: L2è·ç¦»ï¼Œå¯¹é½Studentå’ŒTeacherçš„éšè—çŠ¶æ€
3. **å¯é€‰ï¼šé¢„æµ‹å™¨æŸå¤±ï¼ˆPredictor Lossï¼‰**: æ§åˆ¶ä¿ç•™tokençš„æ¯”ä¾‹

---

## 3. è®­ç»ƒæµç¨‹è¯¦è§£

### 3.1 è®­ç»ƒæ­¥éª¤

```
1. æ•°æ®åŠ è½½
   â†“
2. å›¾åƒç¼–ç ï¼ˆCLIP Vision Encoderï¼‰
   â†“
3. æ–‡æœ¬Tokenization
   â†“
4. ç‰¹å¾èåˆï¼ˆæ–‡æœ¬åµŒå…¥ + è§†è§‰åµŒå…¥ï¼‰
   â†“
5. Studentæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆå¸¦ç¨€ç–åŒ–ï¼‰
   â†“
6. Teacheræ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
   â†“
7. è®¡ç®—æŸå¤±ï¼ˆLM Loss + Alignment Lossï¼‰
   â†“
8. åå‘ä¼ æ’­ï¼ˆä»…æ›´æ–°Studentå‚æ•°ï¼‰
   â†“
9. å‚æ•°æ›´æ–°
```

### 3.2 ç¨€ç–åŒ–æœºåˆ¶

#### **Tokené€‰æ‹©è¿‡ç¨‹**
```python
# ä»£ç ä½ç½®: llava/model/language_model/modelling_sparse_llama.py:221-320

# åœ¨ç‰¹å®šå±‚ï¼ˆpruning_locï¼‰è¿›è¡Œç¨€ç–åŒ–
if layer_idx in self.pruning_loc:  # é€šå¸¸æ˜¯ç¬¬2, 6, 15å±‚
    # 1. è®¡ç®—æ–‡æœ¬-è§†è§‰æ³¨æ„åŠ›
    attn_logits = layer_outputs[2]  # [B, H, L, L]
    
    # 2. æå–æ–‡æœ¬åˆ°è§†è§‰çš„æ³¨æ„åŠ›æƒé‡
    relation_vis_text = attn_logits[:, text_token_idx, v_token_start:v_token_end]
    
    # 3. Top-Ké€‰æ‹©
    _, indices = torch.topk(relation_vis_text, k=retain_tokens, dim=1)
    
    # 4. åˆ›å»ºä¿ç•™mask
    policy = torch.zeros(B, L)
    policy[:, indices] = 1  # ä¿ç•™çš„tokenæ ‡è®°ä¸º1
    
    # 5. åº”ç”¨maskï¼Œåªä¿ç•™é€‰ä¸­çš„token
    selected_hidden_states = hidden_states[policy == 1]
```

#### **ç¨€ç–åŒ–ä½ç½®**
- **å±‚ä½ç½®**: ç¬¬2, 6, 15å±‚ï¼ˆå¯é…ç½®ï¼‰
- **Tokenä¿ç•™æ•°é‡**: 192, 128, 96, 64ï¼ˆå¯é…ç½®ï¼‰
- **ä¿ç•™ç­–ç•¥**: åŸºäºæ–‡æœ¬-è§†è§‰æ³¨æ„åŠ›æƒé‡è¿›è¡ŒTop-Ké€‰æ‹©

---

## 4. ä»£ç ç¤ºä¾‹

### 4.1 è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
# è®­ç»ƒé…ç½®
python llava/train/sparse_train.py \
    --model_name_or_path liuhaotian/llava-v1.5-7b \
    --teacher_model_name_or_path liuhaotian/llava-v1.5-7b \
    --version v1 \
    --data_path ./data/llava_instruct_80k.json \
    --image_folder ./data/images \
    --vision_tower openai/clip-vit-large-patch14-336 \
    --freeze_backbone False \  # å…¨å‚æ•°å¾®è°ƒ
    --tune_mm_mlp_adapter False \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token True \
    --bf16 True \
    --output_dir ./checkpoints/sparsevlm-7b \
    --num_train_epochs 1 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --learning_rate 2e-5 \
    --weight_decay 0.0 \
    --warmup_ratio 0.03 \
    --lr_scheduler_type cosine \
    --gradient_checkpointing True \
    --model_max_length 2048 \
    --deepspeed ./scripts/zero2.json
```

### 4.2 æ•°æ®æ ¼å¼ç¤ºä¾‹

```json
// llava_instruct_80k.json
[
  {
    "id": "1",
    "image": "coco/train2017/000000123456.jpg",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nWhat is in this image?"
      },
      {
        "from": "gpt",
        "value": "This image shows a cat sitting on a windowsill."
      }
    ]
  }
]
```

### 4.3 æ£€æŸ¥å¯è®­ç»ƒå‚æ•°

```python
# ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°æ•°é‡
def count_trainable_parameters(model):
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    return trainable, total

trainable, total = count_trainable_parameters(model)
print(f"å¯è®­ç»ƒå‚æ•°: {trainable/1e9:.2f}B / æ€»å‚æ•°: {total/1e9:.2f}B")
```

---

## 5. å…³é”®è¦ç‚¹æ€»ç»“

### 5.1 è®­ç»ƒå‚æ•°
- **å…¨å‚æ•°å¾®è°ƒ**: ~7Bæˆ–~13Bå‚æ•°
- **å†»ç»“ä¸»å¹²**: ~50-100Må‚æ•°
- **ä»…æŠ•å½±å±‚**: ~4-8Må‚æ•°
- **LoRA**: ~50-100Må‚æ•°

### 5.2 è¾“å…¥æ ¼å¼
- **input_ids**: `[B, L]` Tokenåºåˆ—
- **images**: `[B, 3, 336, 336]` å›¾åƒ
- **inputs_embeds**: `[B, L, 4096]` èåˆåçš„åµŒå…¥ï¼ˆæ–‡æœ¬+è§†è§‰ï¼‰

### 5.3 è¾“å‡ºæ ¼å¼
- **prev_decision**: `[B, L]` Tokené€‰æ‹©å†³ç­–
- **hidden_states**: `[B, L_new, 4096]` ç¨€ç–åŒ–åçš„éšè—çŠ¶æ€ï¼ˆL_new < Lï¼‰
- **loss**: è¯­è¨€å»ºæ¨¡æŸå¤± + ç‰¹å¾å¯¹é½æŸå¤±

### 5.4 è®­ç»ƒç‰¹ç‚¹
- **Teacher-Studentæ¡†æ¶**: ä½¿ç”¨çŸ¥è¯†è’¸é¦
- **åŠ¨æ€ç¨€ç–åŒ–**: æ ¹æ®é—®é¢˜è‡ªé€‚åº”é€‰æ‹©è§†è§‰token
- **å¤šå±‚ç¨€ç–åŒ–**: åœ¨ç¬¬2, 6, 15å±‚è¿›è¡Œtokené€‰æ‹©
- **ç‰¹å¾å¯¹é½**: å¯¹é½Studentå’ŒTeacherçš„éšè—çŠ¶æ€

---

## 6. å‚è€ƒä»£ç ä½ç½®

- **è®­ç»ƒè„šæœ¬**: `llava/train/sparse_train.py`
- **Trainerç±»**: `llava/train/sparse_llava_trainer.py`
- **æ¨¡å‹æ¶æ„**: `llava/model/language_model/sparse_llava_llama.py`
- **è¾“å…¥å¤„ç†**: `llava/model/llava_arch.py:325-509`
- **ç¨€ç–åŒ–é€»è¾‘**: `llava/model/language_model/modelling_sparse_llama.py:221-320`

